---
title: 'Ecological niche models and species distributions'
author: Jes√∫s N. Pinto-Ledezma and Jeannine Cavender-Bares
output:
  pdf_document: default
  html_document: 
    theme: readable
    toc: yes
---

In this lab, we will explore some Correlative models using data at species level (occurrence data points) and environmental data at large spatial scales. We will download occurrence data for oak species distributed in the Americas from [**GBIF**](https://www.gbif.org) and environmental (climatic) data from [**WorldClim**](https://www.worldclim.org).

Part of the explanation of the different algorithms used in this lad was extracted from the vignette of the {**dismo**} R package.

# Set up your data and your working directory

Set up a working directory and put the two data files in that directory. Tell R that this is the directory you will be using, and read in your data:

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
setwd("path/for/your/directory")
```

Install and load the following packages.

```{r, eval = FALSE}

packages <- c("vegan", "raster", "sp", "rgeos", "rworldmap", 
              "corrplot", "dismo", "rgdal", "maptools", "kernlab", 
              "rgbif", "scrubr", "tidyr", "dplyr", "spThin", 
              "corrplot", "sdmvspecies", "mmap") 

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())

if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages], dependencies = TRUE)
}

```

Although we installed several packages in the previous lines a package is missing, the R package {**sdm**}. This package is the engine for modeling the environmental niches (**shadows**) or Grinnellian niches. We will install this package separately because it has several dependencies.

```{r, eval = FALSE}
install.packages("sdm", dependencies = TRUE)
```

```{r, eval = FALSE}
lapply(packages, library, character.only = TRUE)
library(sdm)
```

A last step for setting up the R package {**sdm**} is to use the command **installAll()** this will install any missing package and will guarantee the complete functionality if the package. Yes, this is annoying but is the way of how the package was designed.

```{r, eval = FALSE}
installAll()
```

# Prepare species data

Get some occurrence data for our species from GBIF, directly within R. This may take some time, given the number of occurrences for the selected species. 

NOTE: we need to have an internet connection.

Set the vector with some species scientific names to download occurrence data from **GBIF**.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
spp <- c("Quercus virginiana", "Quercus minima", "Quercus alba", "Quercus fusiformis")
```

Download data using the vector of species names

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
gbif_data <- occ_data(scientificName = spp, hasCoordinate = TRUE, limit = 2000)  

#decrease the 'limit' if you just want to see how many records there are without waiting all the time that it will take to download the whole dataset.

# Save the raw data

dir.create("Data/OCC")
save(gbif_data, file = "Data/OCC/oaks_raw_occ.RDATA")
```

Inspect the downloaded data. Note that we have downloaded data for four species but for simplicity let's pick one species for this lab. I will select *Quercus alba* but you can select any other species.

The returned object is a list of 4 elements (i.e., the number of our species vector), thus, to select the focal species just use **[[position of the species in the vector]]** to get access to the species. For example, I selected *Quercus alba*, so, to get access to this species I just need to type **gbif_data[[3]]**, let's see:

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
#gbif_data
# if, for any species, "Records found" is larger than "Records returned", you need to increase the 'limit' argument above -- see help(occ_data) for options and limitations

# check how the data are organized:
names(gbif_data)
names(gbif_data[[3]]$meta)
names(gbif_data[[3]]$data)
```

Wow, there are a bunch of columns, let's select some columns that are relevant for our purposes.

```{r, eval = FALSE}
# get the columns that matter for mapping and cleaning the occurrence data:
occ_quealb <- gbif_data[[3]]$data[, c("decimalLongitude", "decimalLatitude", 
                                      "scientificName", "occurrenceStatus", 
                                      "coordinateUncertaintyInMeters",
                                      "institutionCode", "references")]
head(occ_quealb)
```
```{r, eval = FALSE}
View(occ_quealb)
```

Now let's clean a little bit the data. First, removing all NA in the occurrence data.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
occ_quealb <- subset(occ_quealb, !is.na(decimalLongitude) & !is.na(decimalLatitude))
```

Let's do some further data cleaning with functions of the {**scrubr**} package (but note this cleaning is not exhaustive!). This will clean some unprobable coordinates in the data.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
occ_quealb <- coord_incomplete(coord_imprecise(coord_impossible(coord_unlikely(occ_quealb))))
```


```{r, warnings = FALSE, message = FALSE, eval = FALSE}
dim(occ_quealb)
## 1999    7
# show some values
#occ_quealb[1:4, c(1:5, 7:10)]
```

Now transform our data to a spatial object, this will help us to visualize better our data.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
quealb_spt <- SpatialPointsDataFrame(coords = occ_quealb[, 1:2], 
                                     data = data.frame(occ_quealb), 
                                     proj4string = CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"))

```

Let's plot the distribution of *Quercus alba* occurrences.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
plot(quealb_spt, col = "darkgreen", pch = 16)
plot(countriesCoarse, add = TRUE, lwd = 2)
```

Seems the data is correct but sometimes the data aggregation can cause some troubles in model predictions (last Wednesday lecture we have talk about the issues with data interpolation and the effect of data aggregation). To solve this issue we will apply a spatial thinning on the species occurrences, to do so, we will use the package {**spThin**}. Given that our species has a broad distribution let's sett a distance threshold of 5 km between occurrence points.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
thinning <- thin(
      loc.data =  occ_quealb, 
      verbose = FALSE, 
      long.col = "decimalLongitude", 
      lat.col = "decimalLatitude",
      spec.col = "scientificName",
      thin.par = 5, # points have at least a minimum distance of 5 km from each other
      reps = 1, 
      locs.thinned.list.return = TRUE, 
      write.files = FALSE, 
      out.dir = "Data/OCC/")
    
thinning <- as.data.frame(thinning)
thinning$Species <- "Quercus_alba"
```

Explore the results of the thinning process. We can see that the number of occurrences of *Quercus alba* decreased from 1999 occurrences to 1105 occurrences. We will use the thinned data for further analyses.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
dim(thinning)
head(thinning)

```

Transform the thinned occurrences to a spatial object

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
thinned_spt <- SpatialPointsDataFrame(coords = thinning[, 1:2], 
                                     data = thinning, 
                                     proj4string = CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"))
```

Now visualize the both spatial objects.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
plot(quealb_spt, col = "red", pch = 15)
plot(thinned_spt, col = "darkgreen", pch = 16, add = TRUE)
plot(countriesCoarse, add = TRUE, lwd = 2)
```

Now let's save the data processed and keep **thinned_spt** for further analyses.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
save(occ_quealb, quealb_spt, thinning, thinned_spt, 
     file = "Data/OCC/quealb_OCC_processed.RData")
rm(gbif_data, occ_quealb, quealb_spt, thinning)

```

Until here we have downloaded and cleaned species occurrence data, now we will prepare the environmental data that is going to be used as predictors for constructing ENM/SDM for our species.

# Prepare environmental data

First we will obtain bioclimatic variables from **WorldClim** and to do that we will use the function '**getData()**' from the package {**raster**}, then we will select which variables are more relevant for explaining the distribution of our species.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
dir.create("Data/Envi")

bios <- raster::getData("worldclim", var = "bio", res = 10, 
                download = TRUE, path = "Data/Envi")

```

You can see that by using the argument **path** within the function **getData()** the bioclimatic variables were also downloaded directly to your hard drive, so you don't need to download the data again. 

Let's explore some details of the bioclimatic data 

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
names(bios)

str(bios[[1]])
```

Now let's plot some bioclimatic variables, let's say BIO1 and BIO12 that are mean annual temperature and annual precipitation, respectively.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
plot(stack(bios$bio1, bios$bio12))
```
Okay, these data is at global level but our species only occur in North America, in order to have a better visualization of the data, let's crop the bioclimatic data to the extent of North America (United States and Canada). 

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
NAs <- subset(countriesCoarse, continent == "North America") # used as extent
```

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
bios_NA <- crop(bios, NAs)
```

Let's visualize the results of cropping the bioclimatic data and overlap the occurrences of *Quercus alba*.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
plot(bios_NA[[1]]) # mean annual temperature
plot(thinned_spt, col = "red", pch = 16, add = TRUE) # add occurrence records
plot(countriesCoarse, lwd = 2, lty = 2, add = TRUE) # add country borders
```
Cool!

# Accessible area

Before any further analysis we need to define the accessible are for our species in the geographical space, remember our species is mostly distributed in the United States with some occurrences in southeast Canada. To define the accessible area let's use a bounding box around the known species occurrences plus ~300 km beyond that bound. This will give us an approximation of the species dispersal within the geographical domain, i.e., North America. This bounding box represent the component **M** in the **BAM** diagram.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
### Species specific accessible area
bb <- bbox(thinned_spt) # bounding box

e <- extent(c(bb[1]-3, bb[3]+3, bb[2]-3, bb[4]+3)) # bounding box + 300 km

p <- as(e, 'SpatialPolygons') # transform to polygon
crs(p) <- crs(bios_NA) # use the geographical reference of the bioclimatic variables

crs(NAs) <- crs(bios_NA)

out <- gIntersection(NAs, p, byid = FALSE) # use NAs to eliminate areas on the sea
```

Now let's plot the accessible area.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
plot(bios_NA[[1]])
plot(p, add = TRUE, lty = 2)
plot(out, add = TRUE, lwd = 2)
#enviSPP <- raster::crop(envi, out)
```

Nice, we can see that the accessible area for our species includes almost all the territory of the United States and southeastern Canada. 

Now crop the bioclimatic data to the extent of the accessible area.

```{r, eval = FALSE}
bios_spp <- raster::crop(bios_NA, out)
bios_spp <- raster::mask(bios_spp, out)

plot(bios_spp[[1]])
plot(thinned_spt, add = TRUE, col = "red", pch = 16)
plot(NAs, add = TRUE, lty = 2)

```

We will use this accessible area as extent for all the posterior analyses.

# Pseudoabsences

To model the environmental niches (ghosts) and to project the species distributions (shadows) we need to have data of species presences and absences, however we only have presence data. What should I do if I don't have absences? That's the question! There is no a straightforward answer for that question, but the most common procedure to get absence data is to generate random points (AKA **pseudoabsences**) on the geographical domain (i.e., the accessible area or M). There is a lot of discussion on how many pseudoabsences we need to use for ENM/SDM, here we will use a conservative number, i.e., twice the number presences.

```{r, eval = FALSE}
set.seed(12345) # Random Number Generation to obtain the same result
# Generate the data
absence <- randomPoints(mask = bios_spp[[1]], 
                        n = round(nrow(thinned_spt)*2, 0), # number of pseudoabsences
                        p = thinned_spt, ext = extent(bios_spp))
```

Now let's combine the presence and pseudoabsence data

```{r, eval = FALSE}
presence <- data.frame(coordinates(thinned_spt)) # presence data
absence <- data.frame(absence) # pseudoabsence data
names(absence) <- names(presence)

presence$Occurrence <- 1 # presence data
absence$Occurrence <- 0 # pseudoabsence data

quealb <- rbind(presence, absence) # combine both information
quealb$Species <- "Quercus_alba"
```

Finally transform the presence-pseudoabsence data to a spatial object and visualize it!

```{r, eval = FALSE}
coordinates(quealb) <- ~ Longitude + Latitude
crs(quealb) <- crs(bios_spp)

quealb
```

```{r, eval = FALSE}
plot(bios_spp[[1]])
plot(quealb[quealb$Occurrence == 1, ], col = "blue", add = TRUE, pch = 16)
points(quealb[quealb$Occurrence == 0, ], col = "red", pch = 16)
```
We can save the processed data and clean the environment a little bit.

```{r, eval = FALSE}
save(presence, absence, quealb, file = "Data/OCC/quealb_PresAbs.RData")

save(bb, e, NAs, out, p, file = "Data/Envi/accessible_area_quealb.RData")

rm(absence, presence, bios, e, out, p, bb)
```

Now, we need to decide which variables are necessary to model the niche (Grinnellian niche or Fundamental niche or the Ghost) for our oak species. Here we can rely the selection to the specialist or use statistical tools or both. 

# Variable selection

Let's use the statistical way. However we can ask Jeannine about which variables are more important for the distribution of oaks.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
quealb_bios <- data.frame(raster::extract(bios_spp, quealb))

quealb_bios <- cbind(data.frame(quealb), quealb_bios)

quealb_bios <- quealb_bios[complete.cases(quealb_bios), ]
quealb_bios <- na.omit(quealb_bios)

head(quealb_bios)
```

One way to select variables is to explore the correlation among the predictors. We can use a threshold of **|r| <0.7** which means that correlated variables below this threshold can be considered as no problematic **(Dormann et al. 2013)**, however you can use a more conservative threshold, such as <0.5.  Let's see.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
# We first estimate a correlation matrix from the predictors. We use Spearman rank correlation coefficient, as we do not know whether all variables are normally distributed.
cor_mat <- cor(quealb_bios[, c(6:24)], method = 'spearman')
```

Let's visualize the correlation matrix.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
corrplot.mixed(cor_mat, tl.pos = "lt", tl.cex = 0.5, number.cex = 0.5, 
               addCoefasPercent = TRUE, mar = c(0, 0, 1, 0))
```

Looks like that several variables are highly correlated, but let's select the ones that are highly correlated with each other and exclude them for further analysis. However inspecting predictor by predictor can take forever, what should we do? Well we have two options:

1. Talk with the specialist about which predictors should we use, or 
2. Let the computer decide 

The simple solution is letting the computer decide, thus, for doing that, we will use an amazing function called '**select07()**' from the package {**mecofun**} that will select the predictors bellow the correlation threshold.

Let's install the package from its repo as it is not currently published in CRAN. Important, a message will appear in your console asking if you would like to update an existing package, please type **3** and press **Enter**. This will order **R** to install just {**mecofun**} and also will avoid any error in the installation.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
# Install the package from source
remotes::install_git("https://gitup.uni-potsdam.de/macroecology/mecofun.git")
```

The function will return a list of three objects:
1. AIC values for each model
2. The correlation matrix
3. A vector of the selected variables.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
library(mecofun)

# Run select07()
 
covar_sel <- select07(X = quealb_bios[, -c(1:5)], # only predictors data
                    y = quealb_bios$Occurrence, # presence-absence data
                    threshold = 0.7) # here you can change the threshold for one

# Check out the structure of the resulting object:
str(covar_sel)
```


```{r, warnings = FALSE, message = FALSE, eval = FALSE}
covar_sel$AIC
covar_sel$cor_mat
covar_sel$pred_sel
```

According to the **select07()** function six predictors best fit the data and also have low correlation. Assuming that this is correct, we will use these variables for further analyses. The selected variables are: "bio15", "bio12", "bio2", "bio10", "bio7" and "bio8".


```{r, warnings = FALSE, message = FALSE, eval = FALSE}
preds <- covar_sel$pred_sel
preds
```

Finally, let's select the bioclimatic variables and plot them 

```{r, warnings = FALSE, message = FALSE, eval = FALSE}

bios_quealb_sel <- stack(bios_spp$bio2, bios_spp$bio7, bios_spp$bio8, 
                         bios_spp$bio10, bios_spp$bio12, bios_spp$bio15)

plot(bios_quealb_sel)

```
Okay, we re ready to build the models for our species.

# Building Environmental Niche Models

To model the environmental niches for *Quercus alba* we will use the R package {**sdm**}. This package is very useful and first we need to create a data object which will save us a lot of time in the future.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
quealbDATA <- sdmData(formula = Occurrence ~ bio2 + bio7 + bio8 + bio10 + bio12 + bio15, 
                      train = quealb, # presence-pseudoabsence data
                      predictors = bios_quealb_sel, # selected covariables
                      crs = crs(bios_quealb_sel)) 
```

Let's explore a little bit the **sdm** object.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
quealbDATA
```

We can see that our object has 1 species, 6 features or predictors and the type of data is presence-absence (well is pseudoabsence) and the number of records.

Now let's model the species-environment relationship or Grinnellian niches. Under {sdm} we can model the species-environment relationships using several algorithms at once, but it depend completely on the power of your computer, so use it at your own risk. To get the complete list of algorithms available in the {**sdm**} package just type **getmethodNames('sdm')** in your console.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
getmethodNames('sdm')
```

Here we will explore the predictions of six algorithms, i.e., Bioclim, Gower, GLMs GAM, SVM and Random forest.

## Fit models

### Bioclim model

The BIOCLIM algorithm computes the similarity of a location by comparing the values of environmental variables at any location to a percentile distribution of the values at known locations of occurrence ('training sites'). 

### Gower model

The Domain algorithm computes the Gower distance between environmental variables at any location and those at any of the known locations of occurrence ('training sites'). For each variable the minimum distance between a site and any of the training points is taken. 

### Generalized Linear Models

A generalized linear model (GLM) is a generalization of ordinary least squares regression. Models are fit using maximum likelihood and by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value. Depending on how a GLM is specified it can be equivalent to (multiple) linear regression, logistic regression or Poisson regression. 

### Random forest

Random Forest (Breiman, 2001) is an extension of the classifictation and regression trees (CART; Breiman et al., 1984). 

### Support Vector Machines or SVM model

Support Vector Machines are an excellent tool for classification, novelty detection, and regression.

Support Vector Machines (SVMs; Vapnik, 1998) apply a simple linear method to the data but in a high-dimensional feature space non-linearly related to the input space, but in practice, it does not involve any computations in that high-dimensional space. This simplicity combined with state of the art performance on many learning problems (classification, regression, and novelty detection) has contributed to the popularity of the SVM (Karatzoglou et al., 2006). 

If you have a powerful computer you can set more cores that will speed the computation time. Additionally, Here we are going to fit 6 models and evaluate them through 2 runs of subsampling, each draw 30 percent of training data as test dataset:

```{r, warnings = FALSE, message = FALSE, eval = FALSE}

#This takes sometime (~3 minutes) please be patient!

quealbSDM <- sdm(Occurrence~., data = quealbDATA, # formula and data
              methods = c("bioclim", "domain.dismo", "glm", "gam", "rf", "svm"), # algorithms
              replication = "sub", test.percent = 30, n = 2, # training-testing subsampling
              parallelSettings = list(ncore = 2, method = "parallel")) # parallel computation

```

Now let's see how well each algorithm fitted the data.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
quealbSDM
```

We can see that all algorithms worked relatively well. The evaluation table shows four metrics of model evaluation, i.e., AUC, COR, TSS and Deviance, each metric has its own properties but I personally like the true-skill statistics (TSS) metric, because it evaluates the matches and mismatches between observations and predictions. So by looking at the column of TSS we can see that the best algorithm that best fitted the occurrence data is **random forest** followed by **gam** and **svm**, respectively. Interestingly, these models also have the highest area under the operating curve (AUC) metric values.

Another way to evaluate the model performance is to plot the AUCs.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
roc(quealbSDM, smooth = TRUE)
```
Until here we have downloaded and processed occurrence data and fitted and evaluated six different algorithms. Now let's predict the Grinnellian niche for *Quercus alba* in the accessible area. We can do that for all six algorithms at once but it will take a very long time, so let's do algorithm by algorithm. In addition, you can use the fitted object (i.e., quealbSDM) to make predictions to other regions (e.g., areas with similar environmental conditions or invadable areas) or to project to other periods of time (i.e., to the future or the past).

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
quealb_ENM_bioclim <- predict(quealbSDM, newdata = bios_quealb_sel, 
                              method = "bioclim", mean = TRUE)
quealb_ENM_bioclim 
```

Now plot the resulting prediction under the **bioclim** algorithm. This map is showing the probability of occurrence of *Quercus alba* under the algorithm **bioclim**, where values closest to 1 indicate higher probability and values near 0 low probability of occurrence. Before visualizing the prediction let's re-scale the prediction to make it more interpretable.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
quealb_ENM_bioclim <- sdmvspecies::rescale(quealb_ENM_bioclim$sp_1.m_bioclim.re_subs)
plot(quealb_ENM_bioclim)
```

Let's repeat the process for the other algorithms.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
quealb_ENM_domain <- predict(quealbSDM, newdata = bios_quealb_sel, 
                              method = "domain.dismo", mean = TRUE)

quealb_ENM_glm <- predict(quealbSDM, newdata = bios_quealb_sel, 
                              method = "glm", mean = TRUE)

quealb_ENM_gam <- predict(quealbSDM, newdata = bios_quealb_sel, 
                              method = "gam", mean = TRUE)

quealb_ENM_rf <- predict(quealbSDM, newdata = bios_quealb_sel, 
                              method = "rf", mean = TRUE)

quealb_ENM_svm <- predict(quealbSDM, newdata = bios_quealb_sel, 
                              method = "svm", mean = TRUE)

# Please, do not pay attention to the warnings...
```

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
quealb_ENM_domain <- sdmvspecies::rescale(quealb_ENM_domain$sp_1.m_domain.dismo.re_subs)
quealb_ENM_glm <- sdmvspecies::rescale(quealb_ENM_glm$sp_1.m_glm.re_subs)
quealb_ENM_gam <- sdmvspecies::rescale(quealb_ENM_gam$sp_1.m_gam.re_subs)
quealb_ENM_rf <- sdmvspecies::rescale(quealb_ENM_rf$sp_1.m_rf.re_subs)
quealb_ENM_svm <- sdmvspecies::rescale(quealb_ENM_svm$sp_1.m_svm.re_subs)

```

Now plot all six predictions

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
quealb_ENM_all <- raster::stack(quealb_ENM_bioclim, quealb_ENM_domain, 
                                quealb_ENM_glm, quealb_ENM_gam, 
                                quealb_ENM_svm, quealb_ENM_rf)
names(quealb_ENM_all) <- c("Bioclim", "Domain", "GLM", "GAM", "SVM", "RF")

```

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
plot(quealb_ENM_all)
```
As you can see all algorithms returned different predictions, for example **Domain** return a very broad prediction while **SVM** a narrow prediction. With this we can conclude that there is no silver-bullet (i.e., perfect algorithm) that allow the prediction of the Grinnellian niches. If you are curious about this issue this [**paper**](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12397) led by  Huijie Qiao can help you.

One potential solution to avoid this issue is to create an **ensemble** model. Basically an ensemble return a prediction by combining results of different modeling algorithms (Ara√∫jo & New, 2007). These ensemble predictions are also know as consensus predictions. Here rather than using the six models we will use the best three models (i.e., rf, gam and svm) and we will use the TSS metric as weights.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
  ### Ensemble prediction - ensemble based on TSS statistics
quealb_ENM_ensemble <- ensemble(quealbSDM, newdata = bios_quealb_sel, 
                                method = c("rf", "gam", "svm"), 
                                setting = list(method = "weighted", stat = "TSS"), 
                                parallelSettings = list(ncore = 2, method = "parallel"))
```

Don't forget to re-scale the model prediction and visualize it.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
quealb_ENM_ensemble <- sdmvspecies::rescale(quealb_ENM_ensemble)
```

Plot the ensemble prediction

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
plot(quealb_ENM_ensemble)
```

*How do you feel about that?*
*Does the ensemble prediction outperform single algorithm predictions?*

You might also want to save the predictions.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
dir.create("Results")
dir.create("Results/ENMs")

writeRaster(quealb_ENM_all, filename = "Results/ENMs/quealb_ENM", 
            suffix = names(quealb_ENM_all), format = "GTiff", 
            bylayer = TRUE, overwrite = TRUE)

writeRaster(quealb_ENM_ensemble, filename = "Results/ENMs/quealb_ENM_Ensemble", 
            format = "GTiff", bylayer = TRUE, overwrite = TRUE)
```

Finally, let's produce "shadows" from the "ghosts".

# Building Species Distributions

## Species geographical distributions

First we need to set a binarization threshold for the model predictions, the threshold (cut-off) is used to transform model predictions (probabilities, distances, or similar values) to a binary score (presence or absence). 

Here we will find the threshold for the **ensemble** prediction, but you can do for each algorithm separately. The threshold we will use intents to maximizes the **sensitivity** (ability of a test to correctly identify presences or true positive rate) plus **specificity** (ability of a test to correctly identify absences or true negative rate) in the predictions.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
dt <- data.frame(as.data.frame(quealbDATA), coordinates(quealbDATA))
head(dt)

prediction <- raster::extract(quealb_ENM_ensemble, dt[, c("Longitude", "Latitude")])

evaluation <- evaluates(df$Occurrence, prediction) # observed versus expected

threshold_sel <- evaluation@threshold_based$threshold[2]

round(threshold_sel, 2)
```

The threshold that maximizes the sensitivity and specificity is **0.36**, we will use that value to produce "shadows" from the "ghosts".

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
quealb_SDM_ensemble <- quealb_ENM_ensemble

quealb_SDM_ensemble[] <- ifelse(quealb_SDM_ensemble[] >= threshold_sel, 1, 0)
```

Plot the distribution of *Quercus alba*

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
plot(quealb_SDM_ensemble)
plot(countriesCoarse, add = TRUE)
```

That's it, we modeled environmental niches (**ghosts**) to produce geographical distributions (**shadows**).

You might also want to save this final raster file in your hard drive.

```{r, warnings = FALSE, message = FALSE, eval = FALSE}
dir.create("Results/SDMs")

writeRaster(quealb_SDM_ensemble, filename = "Results/SDMs/quealb_SDM_Ensemble", 
            format = "GTiff", bylayer = TRUE, overwrite = TRUE)
```

# The challenge

Please respond each question based on the practice.

1. Summarize the data: how many records are there, how many have coordinates, how many records without coordinates have a textual georeference (locality description)?

2. Do you think the observations are a reasonable representation of the distribution (and ecological niche) of the species?

3. There is a best model? Explain your answer.

4. Can we use ENM/SDM to aid the conservation of *Quercus alba*? Explain your answer.

5. Based on the lecture and the practice, what can we conclude?

# References 

Ara√∫jo M. B. and New, M. (2007). Ensemble forecasting of species distributions. Trends Ecol Evol 22(1):42‚Äì47.

Breiman, L., J. Friedman, C.J. Stone and R.A. Olshen, (1984). Classification and Regression Trees. Chapman & Hall/CRC.

Breiman, L. (2001). Random Forests. Machine Learning 45: 5-32. doi:10.1023/a:1010933404324

Dormann, C. F., Elith, J., Bacher, S., Buchmann, C., Carl, G., Carr√©, G., ‚Ä¶ Lautenbach, S. (2012). Collinearity: a review of methods to deal with it and a simulation study evaluating their performance. Ecography, 36(1), 27‚Äì46. doi:10.1111/j.1600-0587.2012.07348.x

Karatzoglou, A., D. Meyer & K. Hornik, 2006. Support Vector Machines in R. Journal of statistical software 15(9).

Vapnik, V., 1998. Statistical Learning Theory. Wiley, New York.



