)
m1 <- brm(
Allowed ~ D.z,
data = lrsmall,
prior = m1priors,
family = "bernoulli",
seed = 123 # Adding a seed makes results reproducible.
)
summary(m1)
gc()
gc()
rep(1:10, 26) + rep(seq(0,120,10), each=20)
unlist(lapply(rep(seq(1, 121, by=10), each=2), function(x) seq(x, x+9)))
(lapply(rep(seq(1, 121, by=10), each=2), function(x) seq(x, x+9)))
rep(1:12, 95) + rep(seq(0, 120, 10), each=20)
(lapply(rep(seq(1, 12, by = 10), each = 1), function(x) seq(x, x+9)))
(lapply(rep(seq(1, 12, by = 1), each = 1), function(x) seq(x, x+12)))
(lapply(rep(seq(1, 12, by = 1), each = 12), function(x) seq(x, x+12)))
(lapply(rep(seq(1, 12, by = 1), each = 12), function(x) seq(x, x+2)))
(lapply(rep(seq(1, 12, by = 1), each = 1), function(x) seq(x, x+2)))
(lapply(rep(seq(1, 12, by = 10), each = 1), function(x) seq(x, x+2)))
(lapply(rep(seq(1, 9, by = 10), each = 1), function(x) seq(x, x+2)))
(lapply(rep(seq(1, 9, by = 1), each = 1), function(x) seq(x, x+2)))
(lapply(rep(seq(1, 9, by = 1), each = 1), function(x) seq(x, x+12)))
(lapply(rep(seq(1, 9, by = 1), each = 12), function(x) seq(x, x+12)))
rep(1:12, 95) + rep(seq(0, 120, 10), each=20)
rep(1:12, 95) + rep(seq(1, 1140, 12), each=20)
unlist(rep(split(seq_len(130), rep(1:13, each=10)), each=2), use.names=FALSE)
rep(split(seq_len(130), rep(1:13, each=10)), each=2)
rep(split(seq_len(130), rep(1:13, each=10)), each=1)
rep(split(seq_len(1140), rep(1:95, each = 12)), each = 1)
tt <- rep(split(seq_len(1140), rep(1:95, each = 12)), each = 1)
tt
(lapply(rep(seq(1, 1140, by = 1), each = 12), function(x) seq(x, x+12)))
(lapply(rep(seq(1, 1140, by = 1), each = 1), function(x) seq(x, x+12)))
(lapply(rep(seq(1, 1140, by = 1), each = 1), function(x) seq(x, x+1)))
library(metaviz)
install.packages("metaviz", dependencies = T)
mozart[1:10, c("d", "se")]
library(metaviz)
viz_forest(x = mozart[1:10, c("d", "se")], study_labels = mozart[1:10, c("study_name")],
summary_label = "Summary effect", xlab = "Cohen d")
mozart[1:10, c("d", "se")]
mozart[1:10, c("study_name")]
viz_forest(x = mozart[1:10, c("d", "se")], study_labels = mozart[1:10, c("study_name")],
xlab = "Cohen d")
install.packages("BayesianTools")
library(BayesianTools)
set.seed(123)
sessionInfo()
ll <- generateTestDensityMultiNormal(sigma = "no correlation")
bayesianSetup = createBayesianSetup(likelihood = ll, lower = rep(-10, 3), upper = rep(10, 3))
iter = 10000
settings = list(iterations = iter, message = FALSE)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Metropolis", settings = settings)
print(out)
summary(out)
plot(out) # plot internally calls tracePlot(out)
par(mar = c(1, 1, 1, 1))
plot(out) # plot internally calls tracePlot(out)
correlationPlot(out)
marginalPlot(out, prior = TRUE)
marginalLikelihood(out)
DIC(out)
MAP(out)
getSample(out, start = 100, end = NULL, thin = 5, whichParameters = 1:2)
iter = 1000
settings = list(iterations = iter, nrChains = 3, message = FALSE)
out <- runMCMC(bayesianSetup = bayesianSetup, sampler = "Metropolis", settings = settings)
print(out)
summary(out)
plot(out)
#getSample(out, coda = F)
gelmanDiagnostics(out, plot = T)
ll = logDensity(x)
ll <- generateTestDensityMultiNormal(sigma = "no correlation")
bayesianSetup = createBayesianSetup(likelihood = ll, lower = rep(-10, 3), upper = rep(10, 3))
ll <- generateTestDensityMultiNormal(sigma = "no correlation")
bayesianSetup = createBayesianSetup(likelihood = ll, lower = rep(-10, 3), upper = rep(10, 3))
## Definition of likelihood function
likelihood <- function(matrix){
# Calculate likelihood in parallel
# Return vector of likelihood valus
}
## Create Bayesian Setup
BS <- createBayesianSetup(likelihood, parallel = "external", ...)
## Run MCMC
runMCMC(BS, sampler = "SMC", ...)
## n = Number of cores
x <- c(1:10)
likelihood <- function(param) return(sum(dnorm(x, mean = param, log = T)))
bayesianSetup <- createBayesianSetup(likelihood, parallel = n, lower = -5, upper = 5)
## give runMCMC a matrix with n rows of proposals as startValues or sample n times from the previous created sampler
out <- runMCMC(bayesianSetup, settings = list(iterations = 100000, startValues = bayesianSetup$prior$sampler(n)))
?updateR
updateR::updateR(admin_password = "tirate@1pozo")
tirate@1pozo
install.packages(as.vector(needed_packages))
updateR::updateR(admin_password = "tirate@1pozo")
install.packages(c("adegraphics", "adespatial", "animation", "backports", "bayou", "BDgraph", "betapart", "BH", "bold", "bookdown", "bridgesampling", "brms", "Brobdingnag", "broom", "brranching", "callr", "car", "carData", "caret", "cartogram", "castor", "checkmate", "chron", "circlize", "cladoRcpp", "class", "classInt", "cli", "clipr", "codetools", "colorspace", "commonmark", "covr", "cowplot", "cramer", "crul", "cubature", "curl", "data.table", "dbplyr", "ddalpha", "deldir", "dendextend", "DepthProc", "DescTools", "devtools", "DHARMa", "dimRed", "DT", "e1071", "earth", "elliptic", "emmeans", "emulator", "eulerr", "fansi", "fitdistrplus", "FNN", "forecast", "FossilSim", "FSA", "future.apply", "fuzzySim", "GA", "gbm", "gclus", "geiger", "geojson", "ggcorrplot", "ggmap", "ggmcmc", "ggpubr", "ggridges", "git2r", "glm2", "glmmTMB", "googleVis", "GPfit", "hisse", "hitandrun", "htmlTable", "httpuv", "httr", "idefix", "IDPmisc", "imager", "inlmisc", "ipred", "jagsUI", "jomo", "jqr", "kableExtra", "knitr", "KnowBR", "labelled", "landscapemetrics", "landscapetools", "later", "lava", "lavaan", "link2GI", "lme4", "lsmeans", "Luminescence", "lwgeom", "magic", "mapview", "markdown", "mclust", "MCMCpack", "mitml", "ModelMetrics", "MODIS", "mvabund", "natserv", "ncf", "nloptr", "nodiv", "np", "OpenMx", "openssl", "optimx", "optparse", "osmdata", "packrat", "pacman", "paleotree", "ParamHelpers", "phylocomr", "phylosignal", "pillar", "pkgbuild", "pkgload", "plotmo", "polyCub", "prabclus", "pracma", "processx", "ps", "psych", "psycho", "qgraph", "quadmesh", "quantreg", "R6", "ranger", "ratematrix", "rbison", "RcppArmadillo", "RcppEigen", "Rdpack", "readr", "readxl", "rebird", "recipes", "reprex", "ResourceSelection", "rgbif", "rgeos", "RgoogleMaps", "rhandsontable", "rio", "ritis", "rjags", "RJSONIO", "rlang", "RLumShiny", "rmapshaper", "Rmpfr", "RNeXML", "robustbase", "rockchalk", "rotl", "roxygen2", "rpart.plot", "rr2", "rrcov", "RSAGA", "rsconnect", "rstan", "rstanarm", "rstantools", "RStoolbox", "rstudioapi", "RVAideMemoire", "rvcheck", "sandwich", "scam", "semTools", "servr", "sf", "sfsmisc", "shiny", "shinydashboard", "shinyFiles", "shinythemes", "slam", "sm", "solrium", "spam", "sparklyr", "spatstat", "spatstat.data", "spatstat.utils", "spData", "spdep", "spocc", "StanHeaders", "stars", "statnet.common", "stplanr", "survey", "sys", "testthat", "TH.data", "tibble", "tidybayes", "tidyr", "tidyselect", "tidytree", "tm", "tmap", "TMB", "traitdataform", "units", "urltools", "VGAM", "wikitaxa", "wiqid", "worrms", "xts", "zoo"))
require(phytools)
?phylo.heatmap
# simulate tree
tree <- pbtree(n=20,scale=1)
# simulate continuous character
X <- fastBM(tree, nsim=5)
phylo.heatmap(tree, X, grid=TRUE)
X
View(X)
tree
install.packages("remotes")
remotes::install_github("thej022214/hisse")
plot(cars)
plot(cars, pch = 16)
hist(cars$speed)
hist
cars
x <- cars
View(x)
str(x)
xx <- as.matrix(x)
str(xx)
View(xx)
plot(x)
plot(x$dist, x$speed)
plot(x$dist ~ x$speed)
library(letsR)
library(sp)
library(spdep)
library(maptools)
library(ecodist)
library(vegan)
library(raster)
library(maptools)
library(rgeos)
library(sp)
aet <- raster("Downloads/ejercicios_datos_UBA/AET.bil")
npp <- raster("Downloads/ejercicios_datos_UBA/npp2.asc")
#Checar que los rasters estén "proyectados"
projection(npp)
projection(aet)
proj4string(aet) <- CRS("+proj=longlat +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +no_defs")
proj4string(npp) <- CRS("+proj=longlat +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +no_defs")
amer_ras <- raster()
# Establecer el "extent" del raster para quedarnos justamente en las coordenadas de América del Sur
extent(amer_ras) <- c(-110,-29,-56,14)
aet_amer  <- crop(aet,extent(amer_ras))
npp_amer  <- crop(npp,extent(amer_ras))
aet_amer1  <- aggregate(aet_amer,2)
npp_amer1  <- aggregate(npp_amer,2)
aet_amer
aet_amer1
aet_amer1_vals <- values(aet_amer1)
aet_amer1_vals <- ifelse(aet_amer1_vals==255,NA,aet_amer1_vals)
aet_amer1_nas <- aet_amer1
values(aet_amer1_nas) <- aet_amer1_vals
carniv_ras_coords <- xyFromCell(carniv_raster, 1:length(values(carniv_raster)))
amsur.carniv <- readShapePoly("amsur_carnivs.shp")
library(raster)
library(maptools)
library(rgeos)
library(sp)
amsur.carniv <- readShapePoly("amsur_carnivs.shp")
amsur.carniv <- readShapePoly("Downloads/ejercicios_datos_UBA/amsur_carnivs.shp")
proj4string(amsur.carniv) <- CRS("+proj=longlat +datum=WGS84")
res(amer_ras) <- 1
carniv_raster <- rasterize(amsur.carniv, amer_ras, field="binomial",fun=function(x,...){length(unique(na.omit(x)))})
carniv_ras_coords <- xyFromCell(carniv_raster, 1:length(values(carniv_raster)))
plot(carniv_raster)
plot(wrld_simpl,add=T)
carniv_ras_aet <- extract(aet_amer1_nas,carniv_ras_coords)
carniv_ras_rich <- values(carniv_raster)
carniv_ras_rich_nonas <- ifelse(is.na(carniv_ras_rich),0,carniv_ras_rich)
carniv_ras_aet_nonas <- ifelse(is.na(carniv_ras_aet),0,carniv_ras_aet)
##### PERMANOVA SES-FOCAL PHYLO #####
FocalPhylo <- na.omit(read.csv("Results/NewTables_SESFields/NonNatives/Alldata_fires_years_NN.csv"))
FocalPhylo$Year = factor(FocalPhylo$Year, levels = unique(FocalPhylo$Year))
FocalPhylo$Experiment = factor(FocalPhylo$Experiment, levels = unique(FocalPhylo$Experiment))
str(FocalPhylo)
library(tidyverse)
library(caret)
# Load the data and remove NAs
data("PimaIndiansDiabetes2", package = "mlbench")
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
# Inspect the data
sample_n(PimaIndiansDiabetes2, 3)
# Split the data into training and test set
set.seed(123)
training.samples <- PimaIndiansDiabetes2$diabetes %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- PimaIndiansDiabetes2[training.samples, ]
test.data <- PimaIndiansDiabetes2[-training.samples, ]
library(MASS)
View(PimaIndiansDiabetes2)
# Fit the model
model <- glm(diabetes ~., data = train.data, family = binomial) %>%
stepAIC(trace = FALSE)
# Summarize the final selected model
summary(model)
# Make predictions
probabilities <- model %>% predict(test.data, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
# Model accuracy
mean(predicted.classes==test.data$diabetes)
full.model <- glm(diabetes ~., data = train.data, family = binomial)
coef(full.model)
step.model <- full.model %>% stepAIC(trace = FALSE)
coef(step.model)
train.data
library(sjPlot)
library(sjlabelled)
library(sjmisc)
library(ggplot2)
install.packages(c("sjPlot", "sjlabelled", "sjmisc"))
library(sjPlot)
library(sjlabelled)
library(sjmisc)
library(ggplot2)
data(efc)
View(efc)
theme_set(theme_sjplot())
# create data frame for fitting model
df <- data.frame(
y = to_factor(y),
sex = to_factor(efc$c161sex),
dep = to_factor(efc$e42dep),
barthel = efc$barthtot,
education = to_factor(efc$c172code)
)
# create binary response
y <- ifelse(efc$neg_c_7 < median(na.omit(efc$neg_c_7)), 0, 1)
# create data frame for fitting model
df <- data.frame(
y = to_factor(y),
sex = to_factor(efc$c161sex),
dep = to_factor(efc$e42dep),
barthel = efc$barthtot,
education = to_factor(efc$c172code)
)
y
# set variable label for response
set_label(df$y) <- "High Negative Impact"
# fit model
m1 <- glm(y ~., data = df, family = binomial(link = "logit"))
plot_model(m1)
df
data(iris)
iris
m2 <- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Species, data = iris)
# variable names as labels, but made "human readable"
# separating dots are removed
plot_model(m2)
summary(m2)
if (require("rstanarm", quietly = TRUE)) {
# make sure we apply a nice theme
library(ggplot2)
theme_set(theme_sjplot())
data(mtcars)
m <- stan_glm(mpg ~ wt + am + cyl + gear, data = mtcars, chains = 1)
# default model
plot_model(m)
# same model, with mean point estimate, dot-style for point estimate
# and different inner/outer probabilities of the HDI
plot_model(
m,
bpe = "mean",
bpe.style = "dot",
prob.inner = .4,
prob.outer = .8
)
}
# default model
plot_model(m)
mtcars
plot_model(
m,
bpe = "mean",
bpe.style = "dot",
prob.inner = .4,
prob.outer = .8
)
# default model
plot_model(m)
x <- read.csv("https://github.com/jesusNPL/BiodiversitySciences/blob/master/Data/Sample.csv")
View(x)
x <- read.csv("BiodiversitySciences/Data/Sample.csv")
x <- read.csv("https://raw.githubusercontent.com/jesusNPL/BiodiversitySciences/master/Data/Sample.csv")
View(x)
x$PD
require(ordinal)
## A simple cumulative link model:
fm1 <- clm(rating ~ contact + temp, data=wine)
summary(fm1)
wine
## A simple cumulative link mixed model:
fmm1 <- clmm(rating ~ contact + temp + (1|judge), data=wine)
summary(fmm1)
?clm
fm1 <- clm(rating ~ temp * contact, data = wine)
fm1 ## print method
summary(fm1)
fm2 <- update(fm1, ~.-temp:contact)
anova(fm1, fm2)
drop1(fm1, test = "Chi")
add1(fm1, ~.+judge, test = "Chi")
fm2 <- step(fm1)
summary(fm2)
coef(fm1)
vcov(fm1)
AIC(fm1)
extractAIC(fm1)
logLik(fm1)
fitted(fm1)
confint(fm1) ## type = "profile"
confint(fm1, type = "Wald")
pr1 <- profile(fm1)
confint(pr1)
## plotting the profiles:
par(mfrow = c(2, 2))
plot(pr1, root = TRUE) ## check for linearity
par(mfrow = c(2, 2))
plot(pr1)
par(mfrow = c(2, 2))
plot(pr1, approx = TRUE)
par(mfrow = c(2, 2))
plot(pr1, Log = TRUE)
par(mfrow = c(2, 2))
plot(pr1, Log = TRUE, relative = FALSE)
## other link functions:
fm4.lgt <- update(fm1, link = "logit") ## default
fm4.prt <- update(fm1, link = "probit")
fm4.ll <- update(fm1, link = "loglog")
fm4.cll <- update(fm1, link = "cloglog")
fm4.cct <- update(fm1, link = "cauchit")
anova(fm4.lgt, fm4.prt, fm4.ll, fm4.cll, fm4.cct)
## structured thresholds:
fm5 <- update(fm1, threshold = "symmetric")
fm6 <- update(fm1, threshold = "equidistant")
anova(fm1, fm5, fm6)
## the slice methods:
slice.fm1 <- slice(fm1)
par(mfrow = c(3, 3))
plot(slice.fm1)
## Another example:
fm.soup <- clm(SURENESS ~ PRODID, data = soup)
summary(fm.soup)
if(require(MASS)) { ## dropterm, addterm, stepAIC, housing
fm1 <- clm(rating ~ temp * contact, data = wine)
dropterm(fm1, test = "Chi")
addterm(fm1, ~.+judge, test = "Chi")
fm3 <- stepAIC(fm1)
summary(fm3)
## Example from MASS::polr:
fm1 <- clm(Sat ~ Infl + Type + Cont, weights = Freq, data = housing)
summary(fm1)
}
?clmm
## Cumulative link model with one random term:
fmm1 <- clmm(rating ~ temp + contact + (1|judge), data = wine)
summary(fmm1)
## Cumulative link mixed model with two random terms:
mm1 <- clmm(SURENESS ~ PROD + (1|RESP) + (1|RESP:PROD), data = soup,
link = "probit", threshold = "equidistant")
mm1
summary(mm1)
soup
if (!require('devtools')) install.packages('devtools')
devtools::install_github('luismurao/nichetoolbox')
require(nichetoolbox)
run_nichetoolbox()
devtools::install_github("ENMGadgets", "narayanibarve")
devtools::install_github("narayanibarve/ENMGadgets")
library(nichetoolbox)
run_nichetoolbox()
load("~/Downloads/array.rdata")
my.data$X
my.data$VCOV
my.data$ID
my.data$D
my.data$nsite
my.data$nyr
my.data$nsp
my.data$nrep
my.data$Ntree
my.data$env
my.data$terrain
my.data$coffee.dummy
my.data$Tree
library(MASS)
library(nlme) # will be loaded automatically if omitted
summary(glmmPQL(y ~ trt + I(week > 2), random = ~ 1 | ID,
family = binomial, data = bacteria))
bacteria
summary(glmmPQL(week ~ trt, random = ~ 1 | ID,
family = binomial, data = bacteria))
?glmmPQL
?glm
summary(glmmPQL(week ~ trt, random = ~ 1 | ID,
family = gaussian, data = bacteria))
summary(glmmPQL(week ~ trt + y, random = ~ 1 | ID,
family = gaussian, data = bacteria))
summary(glmmPQL(week ~ trt + y, random = ~ 1 | ID,
family = Gamma, data = bacteria))
summary(glmmPQL(week ~ trt + y, random = ~ 1 | ID,
family = inverse.gaussian, data = bacteria))
summary(glmmPQL(week ~ trt + y, random = ~ 1 | ID,
family = poisson, data = bacteria))
summary(glmmPQL(week ~ trt + y, random = ~ 1 | ID,
family = quasi, data = bacteria))
summary(glmmPQL(week ~ trt + y, random = ~ 1 | ID,
family = quasibinomial, data = bacteria))
summary(glmmPQL(week ~ trt + y, random = ~ 1 | ID,
family = quasipoisson, data = bacteria))
?glmmPQL
require(DAMOCLES)
DAMOCLES_bootstrap
require(ordinal)
## Cumulative link model with one random term:
fmm1 <- clmm(rating ~ temp + contact + (1|judge), data = wine)
summary(fmm1)
wine
class(wine)
str(wine)
require(bayou)
data(chelonia)
tree <- chelonia$phy
dat <- chelonia$dat
SE <- 0.05
prior <- make.prior(tree,
dists=list(dalpha="dhalfcauchy", dsig2="dhalfcauchy", dsb="dsb",
dk="cdpois", dtheta="dnorm"), param=list(dalpha=list(scale=1),
dsig2=list(scale=1),
dk=list(lambda=15, kmax=200),
dsb=list(bmax=1,prob=1), d
theta=list(mean=mean(dat), sd=2)))
prior <- make.prior(tree, dists=list(dalpha="dhalfcauchy", dsig2="dhalfcauchy",dsb="dsb", dk="cdpois", dtheta="dnorm"), param=list(dalpha=list(scale=1), dsig2=list(scale=1), dk=list(lambda=15, kmax=200), dsb=list(bmax=1,prob=1), dtheta=list(mean=mean(dat), sd=2)))
fit1 <- bayou.mcmc(tree, dat, SE=SE, model="OU", prior, ngen=10000, new.dir=getwd(), plot.freq=2000, ticker.freq=1000)
fit1 <- bayou.makeMCMC(tree, dat, SE=SE, model="OU", prior, ngen=10000, new.dir=getwd(), plot.freq=2000, ticker.freq=1000)
?bayou.makeMCMC
fit1 <- bayou.makeMCMC(tree, dat, SE=SE, model="OU", prior, new.dir=getwd(), plot.freq=2000, ticker.freq=1000)
prior <- make.prior(tree, dists=list(dalpha="dhalfcauchy", dsig2="dhalfcauchy",dsb="dsb", dk="cdpois", dtheta="dnorm"), param=list(dalpha=list(scale=1), dsig2=list(scale=1), dk=list(lambda=15, kmax=200), dsb=list(bmax=1,prob=1), dtheta=list(mean=mean(dat), sd=2)))
prior <- make.prior(tree, dists=list(dalpha="dhalfcauchy", dtheta="dnorm"), param=list(dalpha=list(scale=1), dsig2=list(scale=1), dk=list(lambda=15, kmax=200), dsb=list(bmax=1,prob=1), dtheta=list(mean=mean(dat), sd=2)))
fit1 <- bayou.makeMCMC(tree, dat, SE=SE, model="OU", prior, new.dir=getwd(), plot.freq=2000, ticker.freq=1000)
?make.prior
install_github("khabbazian/l1ou")
require(devtools)
install_github("khabbazian/l1ou")
library(l1ou)
require(surface)
?convertTreeData
# NOT RUN {
## Compute the averages for the variables in 'state.x77', grouped
## according to the region (Northeast, South, North Central, West) that
## each state belongs to.
aggregate(state.x77, list(Region = state.region), mean)
state.x77
## Compute the averages according to region and the occurrence of more
## than 130 days of frost.
aggregate(state.x77,
list(Region = state.region,
Cold = state.x77[,"Frost"] > 130),
mean)
# NOT RUN {
## Compute the averages for the variables in 'state.x77', grouped
## according to the region (Northeast, South, North Central, West) that
## each state belongs to.
aggregate(state.x77, list(Region = state.region), sum)
## example with character variables and NAs
testDF <- data.frame(v1 = c(1,3,5,7,8,3,5,NA,4,5,7,9),
v2 = c(11,33,55,77,88,33,55,NA,44,55,77,99) )
by1 <- c("red", "blue", 1, 2, NA, "big", 1, 2, "red", 1, NA, 12)
by2 <- c("wet", "dry", 99, 95, NA, "damp", 95, 99, "red", 99, NA, NA)
testDF
aggregate(x = testDF, by = list(by1, by2), FUN = "mean")
# and if you want to treat NAs as a group
fby1 <- factor(by1, exclude = "")
fby2 <- factor(by2, exclude = "")
aggregate(x = testDF, by = list(fby1, fby2), FUN = "mean")
chickwts
## Formulas, one ~ one, one ~ many, many ~ one, and many ~ many:
aggregate(weight ~ feed, data = chickwts, mean)
## Formulas, one ~ one, one ~ many, many ~ one, and many ~ many:
aggregate(weight ~ feed, data = chickwts, sum)
aggregate(breaks ~ wool + tension, data = warpbreaks, mean)
warpbreaks
aggregate(cbind(Ozone, Temp) ~ Month, data = airquality, mean)
aggregate(cbind(ncases, ncontrols) ~ alcgp + tobgp, data = esoph, sum)
## Dot notation:
aggregate(. ~ Species, data = iris, mean)
aggregate(len ~ ., data = ToothGrowth, mean)
## Often followed by xtabs():
ag <- aggregate(len ~ ., data = ToothGrowth, mean)
xtabs(len ~ ., data = ag)
getwd()
getwd()
setwd("Documents/GitHub/BiodiversistySciences")
setwd("Documents/GitHub/BiodiversitySciences")
getwd()
dir()
?mean
?sum
?head
x = seq(10, 30)
x
x <- seq(10, 30)
x
data <- factor(c("small", "medium", "large"))
is.factor(data)
matx <- matrix(1:45, nrow = 15)
rownames(matx) <-  LETTERS[1:15]
colnames(matx) <- c("Sample01", "Sample02", "Sample03")
str(matx)
